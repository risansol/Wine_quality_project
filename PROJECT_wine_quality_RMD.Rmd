---
title: "Wine Chemistry and its Quality"
author: "Rita Sánchez"
date: "February 2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Capstone: Wine Quality Project
This report is the last stage of the HarvardX PH125.9x course. The goal of this project is to work with several machine learning algorithms to determine wine quality using its physicochemical characteristics.

```{r libraries, eval=TRUE, echo=FALSE, results='hide', message=FALSE}
library(tidyverse)
library(caret)
library(data.table)
library(readr)
library(ggplot2)
library(corrplot)
library(class)
library(gmodels)
library(dplyr)
library(scales)
library(rpart)
library(rpart.plot) 
library(randomForest)
```


# 1. Introduction

Wine is part of the Mediterranean culture. There is no family gathering or meal with friends in which wine does not play an important role. In last years, there has been a boom in what can be called "wine culture": visits to wineries are organized to learn about its elaboration process or attending wine tasting workshops, in which you try to learn how to appreciate its different facets. In short, the aim is to determine what makes one wine better than another and why. 

This project attempts to answer, briefly, this question by analyzing the different chemical compounds in wine. With this purpose, we use a wine quality dataset from Kaggle [^1]. As predicting wine can be considered a classification problem, we will apply some machine learning classification techniques.

This work is organized as follows: in Section 2, we apply exploratory data analysis techniques; in Section 3, we briefly discuss the proposed methodology, followed by model comparison; and in Section 4, the main finding and conclusion are shown.

[^1]: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009/version/2
P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.

# 2. Data exploration

We load the data and we name our data base data as *wine_data*
```{r download, eval= TRUE}
wine_data <- read.csv("~/Documents/Curso_DataSience(edX)/9-Capstone Project/My own project/wine_quality_project/winequality-red.csv")

```

Now, we can check the dataset structure to know which type of data we are going to work with.

```{r data, eval=FALSE}
# any NA value to clean 
any(is.na(wine_data))
# [1] FALSE  ==> (no NA in the dataset)

# a first view of the data
str(wine_data)

data.frame':	1599 obs. of  12 variables:
 $ fixed.acidity       : num  7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 7.5 ...
 $ volatile.acidity    : num  0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.5 ...
 $ citric.acid         : num  0 0 0.04 0.56 0 0 0.06 0 0.02 0.36 ...
 $ residual.sugar      : num  1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 6.1 ...
 $ chlorides           : num  0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.071 ...
 $ free.sulfur.dioxide : num  11 25 15 17 11 13 15 15 9 17 ...
 $ total.sulfur.dioxide: num  34 67 54 60 34 40 59 21 18 102 ...
 $ density             : num  0.998 0.997 0.997 0.998 0.998 ...
 $ pH                  : num  3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ...
 $ sulphates           : num  0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ...
 $ alcohol             : num  9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ...
 $ quality             : int  5 5 5 6 5 5 5 7 7 5 ...

```

So, our data set is a tidy data frame with 1599 observation in each of the 12 variables and with no NA values.

What attribute is each variable?

* *Fixed acidity*: The fixed acidity is the set of natural acids in wine and it starts in the vineyard, already. Our mouths react instinctively to acidity levels. Hold your mouth open after you sip. If you begin salivating, your mouth is reacting to the acid. The more saliva, the more acid.The predominant fixed acids found in wines are tartaric, malic, citric, and succinic. Their respective levels found in wine can vary greatly but in general one would expect to see 1 to 4 g/dm^3^ tartaric acid; 0 to 8 g/dm^3^ malic acid; 0 to 0,5 g/dm^3^ citric acid; and 0,5 to 2 g/dm^3^ succinic acid.  (g/dm^3^)

* *Volatile acidity*: amount of acetic acid in wine(g/dm^3^), that is considered a fault at higher levels (1,4 g/dm^3 in red and 1,2 g/dm^3 in white) and can smell sharp like nail polish remover or vinegar. Long fermentation (1 month or more), accumulate higher levels of volatile acidity. 

* *Citric acid*: found in small quantities can add some freshness and flavor (g/dm^3^)

* *Residual sugar*: amount of sugar remaining after wine fermentation/production (g/dm^3^). Accordingly to the dataset documentation, it's rare to find wines with less than 1 g/dm^3^, while dry wines range from 1 - 3 g/dm^3^ . On the other hand, wines with more than 45 g/dm^3^ of sugar are categorized as sweet. 

* *Chlorides*: amount of salt in the wine (g/dm^3^)

* *Free sulfur dioxide*: free forms of S02, prevents microbial growth and the oxidation of wine  (mg/dm^3^)

* *Total sulfur dioxide*: amount of free and bound forms of S02  (mg/dm^3^)

* *Density*: the density of water depending on the percent alcohol and sugar content  (g/dm^3^)

* *PH*: describes how acidic or basic a wine is on a scale 0-14 (very acidic: 0, very basic: 14); most wines are between 3-4 on the pH scale. The pH level tells us, also, how intense the acids taste: the higher the ph, the lower the acidity of wine, and vice versa.

* *Sulphates*: salts derived from sulfuric acid, which are basically used in fertilizers, pesticides and pesticides  (potassium sulphate in g/dm^3^).  The  maximum acceptable limit in wine recommended  by the International Organisation of Vine and Wine  is 1 g/L

* *Alcohol*: the percent alcohol content of the wine (% of volumen)

* *Quality*: target variable (based on sensory data, score between 0 and 10)


\newpage

## 2.1. A look at each attribute

Let start with *quality* , the attribute we employ to define how good or bad is a wine. 


```{r quality-hist, eval= TRUE, echo=FALSE, fig.align='center', fig.width=5, fig.height=3}
# histogram to determine the quality distribution

 wine_data %>% 
  ggplot(aes(x = quality)) +
  scale_x_continuous(breaks = seq(3,8,1))+
  geom_bar(fill = "dark grey", color = "black", alpha = 0.8)+
  labs(title ="Quality distribution", x = "Quality")

```

One interesting thing we observe is that are very few wines considered as *bad wines* (i.e., with a quality below 4), a bunch that can be defined as *really good* (with a 7 quality) and is almost anecdotal to find an excellent wine (i.e., with a quality above 8). The vast majority of the wine in this dataset can be considered as *normal* (5 in quality) or *good* (6 in quality).

Considering that quality is the target attribute, we will try to find some relationship with the rest of the attributes in our dataset. So, first, let's take a look on the distribution of the other dataset variables.

**Summary report**
```{r resumen,eval= TRUE, echo=FALSE, fig.width=5, fig.height=5}
summary(wine_data [-12])

```


```{r boxplot I - II , eval= TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=4}

#box-plot one
wine_data[-c(6,7,12)]%>%
  gather(Attributes, value, 1:9) %>% 
  ggplot(aes(x=reorder(Attributes, value, FUN=median), y=value, fill=Attributes)) +
  geom_boxplot(show.legend=FALSE) +
  labs(title="Wines Attributes (Boxplots)") +
    theme(axis.title.y=element_blank(),
        axis.title.x=element_blank()) +
  coord_flip()

# box-plot two


wine_data[c(6,7)]%>%
  gather(Attributes, value, 1:2) %>% 
  ggplot(aes(x=reorder(Attributes, value, FUN=median), y=value, fill=Attributes)) +
  geom_boxplot(show.legend=FALSE) +
  labs(title="Wines Attributes (Boxplots) II") +
  theme(axis.title.y=element_blank(),
        axis.title.x=element_blank()) +
  coord_flip()

```

*Note: the attributes total.suful.dioxide and free.sulfur.dioxide appear in a separate plot due to the presence of very high values that do not allow them to be represented together with the rest of the parameters in the dataset.*

```{r plots vbles, eval= TRUE, echo=FALSE, message=FALSE, fig.align='left', fig.width=7, fig.height=7}

# histograms to know the distribution of the other characteristics
#remove "quality" column, nº12, because it has been already graphed 

wine_data[-12]%>%
  gather(Attributes, value, 1:11) %>%
  ggplot(aes(x=value, fill=Attributes)) +
  geom_histogram(colour="black", show.legend=FALSE) +
  facet_wrap(~Attributes, scales="free_x") +
  labs(x="Values", y="Frequency",
       title="Wines Attributes (Histogram)") 

#density plot for each attribute. Again, we remove "quality"

wine_data[-12]%>%
gather(Attributes, value, 1:11) %>%
  ggplot(aes(x=value, fill=Attributes)) +
  geom_density(colour="dark grey", alpha=0.6, show.legend=FALSE) +
  facet_wrap(~Attributes, scales="free") +
  labs(x="Values", y="Density",
       title="Wines Attributes (Density plots)") 
```

The main findings from the summary and the plots above are summarized as follows:

* *Alcohol*: few outliers and with a distribution concentrated on 9,0%.

* *Sulphates*: distribuited around the mean but showing some skewness, indicating a high number of outliers. 

* *Sulfur dioxide*: this component can be found in its free-form or in bounded forms with other molecules.In the plots above, it is observed that both variable *free.sulfur.dioxide* and *total.sulfur.dioxide* follow the same patterns. In addition, the summary suggests there must be big outliers on data.That's why, these attributes are plotted in a separate boxplot from the rest.

* *Density*: very narrow range variation...in fact, the difference between min and max is less than 0,01...maybe too small to be perceived by an ordinary human. 

* *Chlorides*: very skewed distribution with the majority of the values below 0,1 and with outliers that "squash" the density function.

* *Residual sugar*: Considering that our density plot shows that almost 90% of the distribution is below 4,0, our dataset refers mainly to dry wines, but there are certain outliers.

* *Citric acid*: from the plots above, we can conclude that there are a wide range of "freshness" in our wine selection.

* *Volatile acidity*: The plots show that above 95% of the distribution is below 1,2, so the majority of our dataset are unoaked white wines   

* *Fixed acidity*: presents values in a higher scale that the other two "acidity" attributes, as well as more outliers.
 
* *PH*: plots are consistent with a quasi-normal distribution. They also show that it varies between 3,0 and 3,6, with few exceptions. These values corresponding with the different types of white wine.   

## 2.2. Searching relationships among variables
One we have explore the characteristics of each variable, now we are interested in exploring the relationships among all variables in the dataset, directing the analysis to find out which characteristics are more related to the quality score. A correlation matrix will give this information.

```{r corr-matrix, eval= TRUE, echo=FALSE, message=FALSE, fig.align='center', fig.width=5, fig.height=5}

# correlation matrix

corrplot(cor(wine_data), type="lower", method="number", number.cex = 0.65,
         number.font= 2, tl.cex=0.8)

```
From the correlation matrix we notice the most relevant attributes to quality are **alcohol** and the **volatile acidity**. We also observe some other relevant correlation among other variables and, although they don't contribute directly to the quality, it will be interesting to have a quick look on them.

\newpage
Analyzing the correlation matrix and considering the previous analysis for each attribute, we can draw some assumptions:

* The wine quality is positively correlated to alcohol and negatively correlated to volatile acidity. It makes sense with our previous analysis, since the volatility acidity is responsible for an unpleasent taste in the wine.

* It is observed a significant correlation between density and other attributes such as (from high to low) fixed acidity, alcohol, citric acid and residual sugar.

* Ph is negatively correlated to acidity, since lower values in the ph scale means a higher acidity. As it is a natural fact, it has no sense to explore here this relationships.

* We detect some correlation between sulphates and chlorides, but none of them presents a relevant correlation with quality. 

* Obviously, free and total forms of sulfur dioxide will be related to each other and the same stands for the different acidity types. Regarding this last, we will consider only the volatile acidity in next section due to its high correlation to quality.

### 2.2.1. Correlation to quality
As we have seen in the correlation matrix, *alcohol* and *volatile acidity* are the characteristic with the highest correlation to *quality*. Let's see these relationships with the corresponding boxplots. 


```{r boxplot quality-alcohol, eval= TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=4}

##box plot quality / alcohol

wine_data %>% 
  ggplot(aes(x = factor (quality), y= alcohol))+
  geom_jitter( alpha = .2) +
  geom_boxplot(show.legend=FALSE)+
  labs(title = "Alcohol vs Quality ", x = "Quality", 
       y = "Alcohol (% by volume)")

```

According to this plot, up to a quality level of 6, the average alcohol percentage is around 10%. It is the higher quality levels (7 and 8) that show a significant increase in their average alcohol percentage. Thus, we can point out that relevant correlation between alcohol and quality arises only when we are dealing with the bordering up values. 

```{r boxplot quality-acidit, eval= TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=4}

##box plot quality / volatil.acidity

wine_data %>% 
  ggplot(aes(x = factor (quality), y= volatile.acidity))+
  geom_jitter( alpha = .2) +
  geom_boxplot(show.legend=FALSE)+
  labs(title = "Volatile acidity vs Quality ", x = "Quality", 
       y = "Volatile acidity (g/dm3)")

```

This boxplot shows that the higher the quality, the lower the acidity. Nevertheless, it is not possible to discriminate between the two highest level of quality on the basis of volatile acidity.

### 2.2.2. Other correlations
Now, we plot other attributes that may have some impact on quality in "second round": (i) residual sugar is correlated with density and, in turn, (ii) density is highly correlated with alcohol; and (iii) citric acid is correlated to volatile acidity. 

 
```{r plot density-sugar, eval= TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=3}

wine_data %>% 
  ggplot(aes(x=density, y = residual.sugar))+
  geom_jitter( alpha = .2) +
  geom_smooth()+
  labs(title = "Residual sugar vs Density ", x ="Density (mg/dm3)", 
       y ="Residual sugar (g/dm3)")

```


```{r plot density-alcohol, eval= TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=4}

wine_data %>% 
  ggplot(aes(x=density, y = alcohol))+
  geom_jitter( alpha = .2) +
  geom_smooth()+
  labs(title = "Alcohol vs Density ", x ="Density (mg/dm3)", 
       y ="Alcohol (% by volume)")

```
  
  

```{r plot citric-volatile, eval= TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=4}

wine_data %>% 
  ggplot(aes(x=citric.acid, y = volatile.acidity))+
  geom_jitter( alpha = .2) +
  geom_smooth()+
  labs(title = "Citric acid vs Volatile acidity ", x ="Volatile acidity (g/dm3)", 
       y ="Citric acid (g/dm3)")
```

\newpage

# 3. Modelling
During the exploratory data analysis it has seen that alcohol and volatile acidity are the best attributes to predict wine quality. In this section we discuss several modeling approaches with machine learning to predict wine quality. For that,we generate our training and test set by randomly splitting the data using the following code: 


```{r split_dataset}

set.seed(123)
n <- nrow(wine_data)
trainIndex <- sample(1:n, size = round(0.7*n),replace=FALSE)

training <- wine_data[trainIndex,]
testing <- wine_data[-trainIndex,]

```

So, our training set consist on the 70% of our total dataset (*wine_data*) and the remaining 30% is our test set. Now we can develop different algorithms to predict the wine quality.

To evaluate the performance of the models we will use the followings metrics: 

* **Mean Absolute Error (MAE)**. It represents the average of the absolute difference between the actual and predicted values in the dataset. It measures the average of the residuals in the dataset.

$$MAE = \frac{1}{N}\sum\limits_{i=1}^{N}| y_{i} - \hat{y}| $$

* **Root Mean Squared Error (RMSE)**. It represents the squared root of the average of the squared difference between the original and predicted values in the data set. It measures the standard deviation of residuals.  

$$RMSE = \sqrt{\frac{1}{N}\sum\limits_{i=1}^{N}\ (y_{i} - \hat{y})^2} $$ 

* **R-squared (R^2^)**. It represents the proportion of the variance in the dependent variable which is explained by the regression model. The value of R square will be less than one.

$$R^{2}= \frac{\sum\limits_{i=1}^{N}(\hat{y} - \bar y)^2}{\sum\limits_{i=1}^{N}(y - \bar y)^2} $$ 
Where, $\hat{y}$ is the predicted value of $y$, and $\bar y$ is the mean value of $y$

The lower value of MAE and RMSE implies higher accuracy of a  model. However, a higher value of R square is considered desirable as well.

```{r evaluating_metrics,eval= TRUE, echo=FALSE }

##calculating Mean Absolute Error (MAE)

MAE <- function(actual, predicted) {
  mean(abs(actual - predicted))
}

##calculating Residual Mean Squared Error (RMSE)

RMSE <- function(actual, predicted){
  sqrt(mean((actual - predicted)^2))
}

###  calculating R squared

R <- function(actual, predicted){
  (sum((predicted - mean(actual))^2))/(sum((actual - mean(actual))^2))
}

```


##  3.1. Linear regression
We developed a linear regression model to predict wine quality ($y$) as a function of the differents attributes in the dataset. Since the variables have very different magnitudes, it will be appropriate to normalize these variables in order not to distort the results. Once we have the normalized variables, we estimate the following equation.

$$ y = \beta * X $$
where $X$ is a matrix with 11 columns, one for each attribute analyzed in the previous section normalized.

The results obtained are summarized below: 


```{r lm-model,eval=TRUE, echo=FALSE}

# standarize the data

preProcValues <- preProcess(wine_data, method = c("center", "scale"))
wine_data_sc <- predict(preProcValues, wine_data)

training_sc <- wine_data_sc[trainIndex,]
testing_sc <- wine_data_sc[-trainIndex,]


##with complete dataset ______________________________________________________

#train the model
lm_wine <- lm(quality ~., data = training_sc)

summary(lm_wine)

```

The summary above shows that alcohol have a strong positive relationship with quality, implying that more alcohol will translate into a higher quality of wine. Reversely, there is a strong negative relationship between volatile acidity and quality that means that lower volatile acidity levels corresponding with higher quality of wine. The coefficients for the remaining attributes are in line with the results obtained in the correlation matrix. In short, these results are consistent with the conclusions obtained from the exploratory analysis of our dataset.

The metrics to evaluate the model are:

```{r lm_metrics,eval=TRUE, echo=FALSE }
#predict

lm_wine_hat <- predict(lm_wine, testing_sc)

## model valuation 

mae_lm<-MAE(predicted = lm_wine_hat, actual = testing_sc$quality)
rmse_lm<-RMSE(predicted = lm_wine_hat, actual = testing_sc$quality)
r2_lm<-R(predicted = lm_wine_hat, actual = testing_sc$quality)

knitr::kable(data.frame(metric = c("RMSE","MAE","R_2"),
           value = round(c(mae_lm,rmse_lm,r2_lm),2)))

```



## 3.2. K Nearest Neighbors Regressor 

K-NN algorithm is a supervised machine learning algorithm than can solve classification and regression tasks. The k-NN algorithm uses feature similarity to predict the values of any new data points. It classifies each new data in the corresponding group, depending on whether it has *k* neighbors closer to one group or another. In other words, it calculates the distance of the new element to each of the existing ones, and sorts these distances from smallest to largest to select the group to which it belongs. This group will be, thus, the one with the highest frequency and the smallest distances.

Before implementing the k-NN Regressor, we need to scale the features. With the k-NN algorithm, we measure the distance between the pair of samples that are influenced by the measurement unit. To avoid this, we should normalize the data before implementing the algorithm.

```{r knn, eval= TRUE, echo=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=3}

#repeat; 10-fold cross validation
ctrl<-trainControl(method = "cv", number = 5, classProbs = TRUE)

#Train model

knn_wine <- train(quality~., data = training,
                  method = "knn",
                  preProcess= "center",  ## we should normalize the data
                  trControl = ctrl,
                  metric="RMSE",
                  tuneLength = 5,
                  tuneGrid = data.frame(k= seq(1,100,1)))

ggplot(knn_wine, highlight = TRUE)+
  labs(title = "K-NN model ", x ="k-values", 
       y ="RMSE")

```

We found an optimum model at *k* = `r knn_wine$bestTune`, where the RMSE is minimum. This is the *k* value we use to validate the test set. The evaluation of model performance is as follow:

```{r knn-evaluation, eval= TRUE, echo=FALSE}
knn_wine_hat <- knn(train=training, test=testing,
                    cl = training$quality,k= knn_wine$bestTune)

### calculating table 

confusionMatrix(as.factor(knn_wine_hat),
                as.factor(testing$quality))$table

```

From the result above we notice that this method is not too good in covering the extreme wine quality qualifications. The validation metrics are as follow:

```{r , eval= TRUE, echo=FALSE}

mae_knn <-mean(knn_wine$results$MAE)
rmse_knn <- mean(knn_wine$results$RMSE)
r2_knn<-mean(knn_wine$results$Rsquared)

knitr::kable(data.frame(metric = c("RMSE","MAE","R_2"),
           value = round(c(mae_knn,rmse_knn,r2_knn),2)))

```
\newpage

## 3.3. Regression Tree

The general idea is to build a decision tree and, at the end of each node, obtain a predictor. 

```{r reg_tree,eval= TRUE, echo=FALSE, fig.align='center', fig.width=7, fig.height=5 }

tree_wine <- rpart(quality ~ ., data = training)
rpart.plot(tree_wine, digits = 2, fallen.leaves = TRUE)


```

From the tree above we notice that our predictions barely covering the extreme qualities (both lowest or highest). This idea is reinforced with a look at the summary statistics:

```{r reg_tree_summary,eval= TRUE, echo=FALSE }

tree_wine_hat <- predict(tree_wine, testing)

table(pred= round(tree_wine_hat,1),actual=testing$quality)

```

*Summary statistics of the quality predicted*
```{r, eval= TRUE, echo=FALSE}
summary(tree_wine_hat)
```

*Summary statistics of the true quality*
```{r, eval= TRUE, echo=FALSE}
summary(testing$quality)
```

And here, the metrics to evaluate the model

```{r reg_tree_validation,eval= TRUE, echo=FALSE }

mae_tree<-MAE(predicted = tree_wine_hat, actual = testing$quality)
rmse_tree<-RMSE(predicted = tree_wine_hat, actual = testing$quality)
r2_tree<-R(predicted = tree_wine_hat, actual = testing$quality)

knitr::kable(data.frame(metric = c("RMSE","MAE","R_2"),
           value = round(c(mae_tree,rmse_tree,r2_tree),2)))

```

## 3.4. Random Forest

This algorithm is a very popular machine learning approach that addresses the shortcomings of decision trees. The goal is to improve prediction performance and reduce instability by *averaging* multiple decision trees. Each decision trees is trained with a random sample drawn from the original training data by bootstrapping. This means that each tree is trained on slightly different data.

The general idea is to generate many predictor, each using regression (or classification trees) and then forming a final prediction based on the average prediction of all these trees.

Let's build the model and take a look at it:

```{r rf, eval= TRUE, echo=FALSE}

### with function
rf_wine <- randomForest(quality ~ ., data=training)
rf_wine

```
 
 
We can see that 500 trees were built and the model randomly sampled 3 predictors at each split. In the following plot, we can see also the number of trees that minimizes the error

```{r rf_plot,eval= TRUE, echo=FALSE, fig.align='center', fig.width=5, fig.height=4 }

#Random forest error curve

plot(rf_wine, main = "Random forest error curve")
```
\newpage

Now we test the model on the test data set:

```{r rf_test, eval= TRUE, echo=FALSE}

rf_wine_hat <- predict(rf_wine, testing )
table(pred= round(rf_wine_hat,1),actual=testing$quality)

```

*Summary statistics of the quality predicted*
```{r, eval= TRUE, echo=FALSE}
summary(rf_wine_hat)
```

*Summary statistics of the actual quality*
```{r, eval= TRUE, echo=FALSE}
summary(testing$quality)
```

From the above results we can see that the model now captures the higher quality a bit better than the regression tree, but still does not detect any wines from the lower or the highest quality. However, with this method we can identify the important variables to check when choosing a wine.
 
 
```{r rf_plot_importance, eval= TRUE, echo=FALSE, fig.align='center', fig.width=7, fig.height=5}

varImpPlot(rf_wine,type=2, main = "Random Forest: Variable importance")
```

It is interesting to note that with this method, the attribute *sulphates* is among the three most relevant for wine quality. This was not observed in the correlation matrix, although in the linear regression model it was a significant variable but with a lower coefficient than the one estimated for the *alcohol* and the *volatility.acidity*. 

Finally, these are the metrics to evaluate the model

```{r rf_validation, eval= TRUE, echo=FALSE}

mae_rf <-MAE(predicted = rf_wine_hat, actual = testing$quality)
rmse_rf<- RMSE(predicted = rf_wine_hat, actual = testing$quality)
r2_rf <- R(predicted = rf_wine_hat, actual = testing$quality)


knitr::kable(data.frame(metric = c("RMSE","MAE","R_2"),
           value = round(c(mae_rf,rmse_rf,r2_rf),2)), align = "l")

```
\newpage

## 3.5. Comparing result

After trying different algorithms to determine the wine quality based on its chemical composition, let's compare the main validation metrics

```{r comparing_table, eval= TRUE, echo=FALSE }
valuation_result <- data.frame(model  = c("Linear Regression", "Knn", "Regression Tree", "Random Forest"), 
                               RMSE = round (c(rmse_lm, rmse_knn,rmse_tree,rmse_rf),2),
                               MAE = round (c(mae_lm, mae_knn,mae_tree,mae_rf),2),
                               R_2= round (c(r2_lm, r2_knn,r2_tree,r2_rf),2))

knitr::kable(valuation_result)
```

From the table above we can conclude that the more effective algorithm to determine the wine quality trough its chemical composition is the *Random Forest*: it shows the lowest value in *RMSE* and *MAE* and its *R^2^* is also the highest, but at a very low level.

In all cases, the algorithms fail to detect the most extreme quality categories, both the highest and the lowest, although with the random forest we slightly improve the ability to detect wines of superior quality, but not those that could be described as "exceptional".

# 4. Conclusion

In this project we have worked with some simple machine learning algorithms trying to approximate the quality of a wine based on its chemical characteristics. 

Along the process we have observed significant limitations to detect wines with the lowest qualities (rated 3 or 4) or with the highest qualities (rated 7 or 8). This may be due to the specific nature of the database. In the Section 2, we have noted that out of `r length(wine_data$quality)` data, `r sum(with(wine_data,quality == "5"))` had quality *5* and `r sum(with(wine_data,quality == "6"))` had quality *6*. In other words, almost `r round(100*((sum(with(wine_data,quality == "5")) + sum(with(wine_data,quality == "5")))/ length(wine_data$quality)),1)`% of the total sample is concentrated in just two quality categories. This limits the learning ability of the algorithms to detect wines in the most extreme quality categories.

What is certain is that winemaking processes have improved substantially in recent decades, which explains why there are a large number of wines in the market with a more than acceptable quality. At this point, it is revealing the advice given to me by a sommelier friend: "the best wine is the one you like the most".

All that remains is to open a bottle and enjoy a glass of our favorite wine, the one to which we will always assign the highest quality, regardless of its chemical composition. Cheers!



